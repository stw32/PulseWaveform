# Constants:

lab.time = "time (s)"
lab.ppg = "Detrended"
config.rate = 0.85
const.pi = 3.1415926535897932384626433


# HED functions:

# 1. GetParticipants (ISO dataset specific)
# 2. GetDirec (ISO dataset specific)
# 3. GetPairs (ISO dataset specific)
# 4. Undetrend (ISO dataset specific)
# 5. FactorAdjust (ISO dataset specific)
# 6. OffsetAdjust (ISO dataset specific)
# 7. FindUndetrendingParams (ISO dataset specific)
# 8. AddOutput
# 9. FindStartParams
# 10. FindWithinParams
# 11. make_matrix
# 12. extractOutput
# 13. FixOutput
# 14. UpdateBeat
# 15. FixBaseline
# 16. PlotFits
# 17. osnd_fit
# 18. ArrangeOutputs
# 19. model2.GetSegment
# 20. model2.Excess
# 21. model2.Peak
# 22. model2.SubtractExcessPeak
# 23. model2.ChiSq3
# 24. model2.ChiSq4
# 25. model2.Rebuild2
# 26. model2.Excess.Inv2
# 27. model2.FIX_PAR3
# 28. model2.FixParams3
# 29. simplex.MakeSimplex2
# 30. simplex.MakeSimplex3
# 31. simplex.Run2
# 32. simplex.HypoCentre
# 33. simplex.SortHighLow
# 34. PlotRejects
# 35. PlotWavesCarriedForward



GetParticipants <- function(direc){
  string_list <- list.files(path = direc)
  string_list <- string_list[-119]
  string_list <- substr(string_list, 0, 5)
  rejected_ts <- c("AL826", "AO139", "AP493", "AU602", "AZ985", "AZ883")
  to_remove <- c()
  for(i in 1:length(rejected_ts)){
    tmp <- which(string_list == rejected_ts[i])
    if(length(tmp) > 0){to_remove <- c(to_remove, tmp)}
  }
  string_list <- string_list[-to_remove]
  return(string_list)
}


GetDirec <- function(run, Participants, dir){
  subjectID <- Participants[run]
  participant_number <- run
  direc <- paste(dir, subjectID, sep = "")
  scan_no <- list.files(direc)[grep(list.files(direc), pattern = "scan")]
  # If it's an 'ISO_ONLY' file another step is needed:
  if(length(scan_no) < 1){
    a <- list.files(substr(direc, 1, nchar(direc)-5))
    b <- grep(a, pattern = subjectID)
    direc <- paste(dir, a[b], sep = "")
    scan_no <- list.files(direc)[grep(list.files(direc), pattern = "scan")]
  }
  direc <- paste(direc, "/", scan_no, sep = "")
  physio_file <- list.files(direc)[grep(list.files(direc), pattern = "physio")]
  direc <- paste(direc, "/", physio_file, sep = "")
  temp <- c(direc, subjectID)
  return(temp)
}



GetPairs <- function(direc, run_order, participant_number, subjectID){
  
  str <- direc
  direc <- list.files(path = direc)
  
  # Refine direc:
  direc <- direc[-grep("REST", direc)]                                 # removing irrelevant files... 
  direc <- direc[-grep(".fig", direc)]  
  direc <- direc[-grep(".ecgpk", direc)]  
  direc <- direc[-grep(".resppk", direc)]  
  direc <- direc[-grep(".hrrv", direc)]  
  direc <- direc[grep("ECG", direc)]                                  
  
  dose_numbers <- parse_number(direc)                                  # extracting numbers from files
  direc <- direc[rev(order(dose_numbers, decreasing = T))]             # Arrange files in numerical order
  if(length(direc) < 6){message("Warning: Some time series are missing from this folder")}
  # Extract dose order:
  dose_order <- run_order[which(run_order$subj_ID == subjectID), 3]    
  dose_order <- as.numeric(unlist(strsplit(dose_order,",")))
  
  # Arrange files in order of escalating dose:
  direc <- direc[rev(order(dose_order, decreasing = T))]
  
  # Randomized pairing (probability 0.5):                              # Setting the same seed and having individual participant numbers will mean randomization is reproducible and varied across participants
  set.seed(32)
  
  if(rnorm(participant_number)[participant_number] > 0){
    # Pair 1:5 and 2:6
    pair1 <- c(direc[5], direc[1])
    pair2 <- c(direc[6], direc[2])
  }else{
    # Pair 1:6 and 2:5
    pair1 <- c(direc[6], direc[1])
    pair2 <- c(direc[5], direc[2])
  }
  
  pairs <- list(pair1, pair2)
  
  # Return pairs:
  return(pairs)
}



UnDetrend <- function(ppg,factor=0,offset=1)    
{
  k <- offset * (1-factor)
  n <- nrow(ppg)
  result <- (1:n)*0
  result[1] = ppg[1,2]
  
  for (i in 2:n)
  {
    result[i] = ppg[i,2] - ppg[i-1,2] * factor - k + result[i-1]
  }
  
  return(result)
}


FactorAdjust <- function(data, factorCutoff, ppg, u, beat, a., test, gs=gs, beatTime, nextTime, plot = T){
  
  # Calculate the Gradient of the tail of the beat:
  tail <- c(data[nrow(data), 2], data[nrow(data)-1, 2], data[nrow(data)-2, 2], data[nrow(data)-3, 2], 
            data[nrow(data)-4, 2])  
  
  # This will determine whether the gradient is initially found to be positive or not, which will influence the events that follow. 
  
  # So make initial assessment based on last 5 values. 
  # If the gradient of last 5 values is positive, use values from before the last 5. 
    # If the last 5 were positive due to noise, this will make sure it is counted as negative from that point on. 
    # If the last 5 were genuinely positive, then the positivity should extend beyond the last 5
  # If the gradient of last 5 values is negative, use the last values. 
    # Don't tend to see falsely negative values, only falsely positive. 
    # Choosing values earlier than the last 5 
  
  # There are actually two criteria I'm now using to iterate the while loop. The first is the gradient, the second 
  # is a check to make sure the minimum of the data segment is not in the middle 50% i.e due to the notch. 
  
  # This means that the second gradient check and subsequent ifelse statements are necessary, because positive and negative gradients
  # could enter the while loop if only the low notch criteria is met. 
  
  # As far as I can tell, there needs to be an initial check of the last 5 data segments to determine which section of the tail will then
  # be iterated on in the while loop. 
  
  # The reasoning for this is as follows:
  
  #  Basically, at the end of waves you can have 4 shapes:
     # 1. prolonged negative slope (morphologically normal)
     # 2. prolonged positive slope (due to heavy detrending)
     # 3. ~5 positive slope coming out of the notch, followed by ~5 negative slope as a rather short tail i.e an n shape (these tend to be ISO waves as they are short due to high HR) 
     # 4. ~5 positive slope at the end of the tail, preceded by a prolonged negative slope i.e a v shape (due to tail noise)
  
  # With an initial assessment determining the iterations that follow:
  # 1. and 2. will be processed the same regardless of which of the last 10 datapoints the gradient is taken from
  # 3. In the initial 'last 5' datapoint assessment, these will be found to be negative and thus will continue being iterated on based on the last 5 datapoints (which is what we want)
  # 4. In the initial 'last 5' datapoint assessement, these will be found to be positive, and so will be iterated on based on NOT the last 5 datapoints (also what we want)
  
  tail <- rev(tail)
  xx. <- 1:length(tail)
  y. <- lm(tail~xx.)
  # Adjust the factor value until the gradient of the tail reaches an appropriate threshold, and the min value is not the notch:
  factor_value <- 1  
  while(y.[[1]][2] > factorCutoff | (which.min(data[, 2]) > quantile(1:nrow(data))[[2]] & which.min(data[, 2]) < quantile(1:nrow(data))[[4]]) ){    
    if(factor_value < 0.7){break}
    factor_value <- factor_value - 0.01
    ppg2 <- ppg
    ppg2[, 2] <- u(ppg,factor=factor_value,offset=1)
    #beatTime <- beat[test + a., 1]   # same adjustments made here as first two lines
    #nextTime <- beat[test + (a. + 1), 1] 
    seg <- c(which(ppg$`time (s)` ==  beatTime), 0, which(ppg$`time (s)` == nextTime))
    data <- gs(ppg2,seg)
    if(plot == TRUE){plot(data, pch = 19)}
    # If the gradient is positive, ignore the last 5 values... 
    if(y.[[1]][2] > 0){
      tail <- c(data[nrow(data)-5, 2], data[nrow(data)-6, 2], data[nrow(data)-7, 2], data[nrow(data)-8, 2], 
                data[nrow(data)-9, 2], data[nrow(data)-10, 2], data[nrow(data)-11, 2], data[nrow(data)-12, 2],
                data[nrow(data)-13, 2], data[nrow(data)-14, 2])
    }else{
      tail <- c(data[nrow(data), 2], data[nrow(data)-1, 2], data[nrow(data)-2, 2], data[nrow(data)-3, 2], 
                data[nrow(data)-4, 2], data[nrow(data)-5, 2], data[nrow(data)-6, 2], data[nrow(data)-7, 2], 
                data[nrow(data)-8, 2], data[nrow(data)-9, 2])
    }
    tail <- rev(tail)
    xx. <- 1:length(tail)
    y. <- lm(tail~xx.)
  }
  
  return(factor_value)
}


OffsetAdjust <- function(ppg3, ppg, u = UnDetrend, factor_value, plot = F){
  if(factor_value == 1){   # if no changes in factor value were needed, no need to correct offset
    print("no adjustment needed")
    return(1)
  } 
  vv. <- ppg3[, 1]      
  yv. <- lm(ppg3[, 2]~vv.)
  offset_value <- 1
  while(yv.[[1]][2] > 0){
    if(yv.[[1]][2] > 5){
      offset_value <- offset_value + 1 
    }else{
      if(yv.[[1]][2] > 1){
        offset_value <- offset_value + 0.5
      }else{
        offset_value <- offset_value + 0.05
      }
    }
    ppg3 <- data.frame(ppg[,1], u(ppg,factor=factor_value,offset=offset_value))
    vv. <- ppg3[, 1]      
    yv. <- lm(ppg3[, 2]~vv.)
    if(plot == TRUE){
      if(yv.[[1]][2]>0){plot(ppg[,1],u(ppg,factor=factor_value,offset=offset_value), type = "l")}
      if(yv.[[1]][2]>0){abline(a = yv.[[1]][1], b = yv.[[1]][2], col = "red")}
      if(yv.[[1]][2]>0){print(yv.[[1]][2])} 
    }
  }
  return(offset_value)
}


FindUndetrendingParams <- function(direc, gs = model2.GetSegment, u = UnDetrend, oa = OffsetAdjust, fa = FactorAdjust, factorCutoff = 0, sr = samplingRate, pk_thrshd, pairs, plot = T){
 
  if(plot == TRUE){p <- TRUE}else{p <- FALSE}
  
  # Factor Value Adjustment (4 waves from each time series):
  
  factor_value_vec <- c()
  for(ps in 1:2){
    if(ps == 1){pair <- pairs[[1]]}else{pair <- pairs[[2]]}
    for(pr in 1:2){
      new_direc <- paste(direc, "/", pair[pr], sep = "")
      ppg <- read.csv(new_direc, sep = "")   
      ppg <- data.frame(
        time = (0:(nrow(ppg)-1)) / samplingRate,
        ppg = ppg[,1]
      )
      names(ppg)[1] <- "time (s)"
      names(ppg)[2] <- "Detrended"
      
      # Find beats:    
      n <- dim(ppg)[1]
      vpg <- ppg[2:n,2] - ppg[1:(n-1),2]
      beat <- data.frame(ppg[which(vpg[1:(n-1)] < pk_thrshd & vpg[2:n] >= pk_thrshd),1])  
      if(nrow(beat) < length(vpg)/(sr*2)){    # if number of beats found suggests a HR of < 30bpm, trigger warning
        message("Warning: Minimal peaks found - consider resetting vpg peak threshold")
        Sys.sleep(10)
      }
    
      t_value <- c()
      for(i in 1:4){
        print(i)
        # We don't know the exact time of onset of iso at this point, but we can infer the IBI from beat and identify
        # waves around the minimum point:
        
        # Find rolling median:
        pre_ibi <- abs(beat[1:(nrow(beat)-1), 1] - beat[2:nrow(beat), 1])
        meds <- rollmedian(pre_ibi, k = 19)
        # plot(meds)
        
        # Failsafes in case the minimum is close to the end / beginning
        min <- which.min(meds)
        if((min - 10) < 1){min = 11}
        if((min + 10) > length(meds)){min = length(meds) - 11}
        test <- round(quantile((min-10):(min+30)))[[i]]
        
        # Extract the relevant beat:
        beatTime <- beat[test, 1]   
        nextTime <- beat[(test + 1), 1]
        seg <- c(which(ppg$`time (s)` ==  beatTime), 0, which(ppg$`time (s)` == nextTime))
        data <- gs(ppg,seg)
        
        # Since we are using a less robust method to find beats, there is a chance of multi-beat segments:
        # Detect them and choose the next segment if so...:
        a. <- 0
        while (nrow(data) > sr*1.5 | nrow(data) < (0.375*sr) | 
               sum(c(order(data[, 2], decreasing = T)[1:5][-1], order(data[, 2], decreasing = T)[1:5][1]) > 20) > 0 ){    # this line insures that the max (5) points of the data are not far apart in time (as would be the case if two peaks were present)
          beatTime <- beat[test + a., 1]  
          nextTime <- beat[test + (a. + 1), 1]
          seg <- c(which(ppg$`time (s)` ==  beatTime), 0, which(ppg$`time (s)` == nextTime))
          data <- gs(ppg,seg)
          a. <- a. + 1
        }
        if(a. != 0){
          a. <- a. - 1
        }
        
        if(plot == TRUE){plot(data, pch = 19)}
        t_value[i] <- fa(data, factorCutoff, ppg, u, beat, a., test, gs, beatTime, nextTime, plot = p)
      }

      factor_value_vec <- c(factor_value_vec, t_value) 
    }
  }
  
  # Find the 2nd from minimum factor value (and check which dose level time series it comes from):
  min2 <- order(factor_value_vec)[2]  # ascending order
  if(sum(min2 == 5:8) > 0 | sum(min2 == 13:16) > 0){
    message("Warning: factor value taken from 0mg time series")
    Sys.sleep(8)
    min1 <- order(factor_value_vec)[1]
    if(sum(min1 == 5:8) > 0 | sum(min1 == 13:16) > 0){
      message("though minimum factor value found in 2mg time series")
      Sys.sleep(8)
    }
  }
  factor_value <- factor_value_vec[min2]
  
  # Offset Value Adjustment:
  
  offset_value <- c()
  for(ps in 1:2){
    if(ps == 1){pair <- pairs[[1]]}else{pair <- pairs[[2]]}
    for(pr in 1:2){
      new_direc <- paste(direc, "/", pair[pr], sep = "")
      # Load time series:
      ppg <- read.csv(new_direc, sep = "")   
      ppg <- data.frame(
        time = (0:(nrow(ppg)-1)) / samplingRate,
        ppg = ppg[,1]
      )
      names(ppg)[1] <- "time (s)"
      names(ppg)[2] <- "Detrended"
      # Adjust factor:  
      ppg3 <- data.frame(ppg[,1],UnDetrend(ppg,factor=factor_value,offset=1))
      # Adjust offset:
      offset_value <- c(offset_value, oa(ppg3, ppg, u = UnDetrend, factor_value, plot = p))
    }
  }
  
  offset_value <- median(offset_value)
  values <- c(factor_value, offset_value)
  return(values)
}



AddOutput <- function(beat){
  beat$First      = 1:nrow(beat) * 0
  beat$Last       = 1:nrow(beat) * 0
  beat$Baseline   = 1:nrow(beat) * 0
  beat$Baseline2  = 1:nrow(beat) * 0
  beat$STime      = 1:nrow(beat) * 0
  beat$SAmplitude = 1:nrow(beat) * 0
  beat$SWidth     = 1:nrow(beat) * 0
  beat$DTime      = 1:nrow(beat) * 0
  beat$DAmplitude = 1:nrow(beat) * 0
  beat$DWidth     = 1:nrow(beat) * 0
  beat$NTime      = 1:nrow(beat) * 0
  beat$NAmplitude = 1:nrow(beat) * 0
  beat$NWidth     = 1:nrow(beat) * 0
  beat$config.rate= rep(0.75, nrow(beat))  
  return(beat)
}


FindStartParams <- function(batch_number, beats_in, beat, ppg, gs = model2.GetSegment, e = model2.Excess, sep = model2.SubtractExcessPeak, o_points = inflexX[o_orig], wuv = wuv, inflexX = inflexX, all_beats = FALSE, plot = FALSE){
  nBeats <- nrow(beat)
  seg <- c(0,0,0)
  if((batch_number*beats_in) > nBeats){
    print("Batch and beat values request more beats than are in time series, defaulting to max number of beats")
    maxn <- nBeats 
  }else{
    maxn <-(batch_number*beats_in)
    if(all_beats == TRUE){maxn <- maxn + (nrow(beat) - maxn)}
  }
  
  # You can check 02 and 0 points imported with the following plot (useful for debugging):
  # plot(5900:6500, ppg$Detrended[5900:6500], type = "l")
  # points(o_points, rep(0, length(o_points)))
  # points(inflexX[wuv$o2], rep(0, length(wuv$o2)), col = "red")
  
  for (i in 1:maxn){  
      
    # Find Beat:
    beatTime <- beat[i,1]
    current_o <- which(ppg[, 1] == beatTime)
    # Find the minimum o_point that is after the current o point
    next_o <- min(which(o_points > current_o))
    next_o <- o_points[next_o]
    # Make sure that through rounding you haven't chosen the same o twice
    if ((next_o - current_o) < 5){
      next_o <- min(which(o_points > current_o)) + 1
      next_o <- o_points[next_o]
    }
    # Find the closest ppg time value to the next_o (perhaps a rounding issue was causing problems before...?)
    nextTime <- ppg[round(next_o), 1]
    seg <- c(which(ppg$`time (s)` ==  beatTime), 0, which(abs(ppg[, 1] - nextTime) == min(abs(ppg[, 1] - nextTime))))
    data <- gs(ppg,seg)
    if(plot == TRUE){plot(data)}
      
    tStart <- ppg[seg[1],1]
    yPrev <- ppg[max(seg[1]-1,1),2]
    
    amp <- max(data[, 2]) - min(data[, 2])
    constant <- 0.1092254*amp
    baseline <- min(data[,2]) - constant    
    residue <- e(data[,2], ppg[seg[1]-1,2], -0) ## - 0?     
    
    count <- nrow(data)
    excess <- 1:count * 0.0
    excess[1] = data[1,2] - (baseline + config.rate*(yPrev-baseline))
    for (j in 2:count){
      excess[j] = data[j,2] - (baseline + config.rate*(data[j-1,2]-baseline))
    }
    rm(count)
    rm(j)
    # plot(data[,1],excess, type = "l")
    par <- 1:10 * 0.
    par[1] = baseline
    residue <- excess  
    
    # S peak
    peak.w <- which(data[,1] > beat[i,1]-0.2 & data[,1] < beat[i,1]+0.2)  
    peak.t <- data[peak.w,1]
    peak.y <- residue[peak.w]
    # plot(data[,1],excess)   
    # lines(peak.t,peak.y)
    par[3] <- max(peak.y)      # height of the S peak
    par[2] <- peak.t[which(peak.y==par[3])]   # timing of the S peak
    par[4] <- 0.25    # width of the S-peak, a priori
    rm(peak.w,peak.t,peak.y)
    residue <- sep(data[,1],residue,par[2:4])
    # plot(data[,1],excess)
    # lines(data[,1],residue)  
    
    # D peak
    peak.w <- which(data[,1] > beat[i,1]+0.3 & data[,1] < beat[i,1]+0.6)  # this finds a range in where to find the peak of d
    peak.t <- data[peak.w,1]     # this finds the time corresponding to the peak
    peak.y <- residue[peak.w]
    # plot(data[,1],excess)   
    # lines(peak.t,peak.y)
    par[6] <- max(peak.y)     
    par[5] <- peak.t[which(peak.y==par[6])]   
    par[7] <- 0.25  
    rm(peak.w,peak.t,peak.y)
    residue <- sep(data[,1],residue,par[5:7])
    # plot(data[,1],excess)
    # lines(data[,1],residue)
    
    # N peak
    t <- par[2] + c(0.25,0.75) * (par[5]-par[2])
    peak.w <- which(data[,1] > t[1] & data[,1] < t[2])
    peak.t <- data[peak.w,1]
    peak.y <- residue[peak.w]
    # plot(data[,1],excess)   
    # lines(peak.t,peak.y)
    par[9] <- max(peak.y)
    par[8] <- peak.t[which(peak.y==par[9])]
    par[10] <- 0.25
    rm(peak.w,peak.t,peak.y,t)
    residue <- sep(data[,1],residue,par[8:10])
    # plot(data[,1],excess)
    # lines(data[,1],residue)
    
    # Store parameters
    w <- seg[1]:seg[3]
    ppg$Baseline[w] <- baseline
    ppg$Excess[w] <- excess
    ppg$Residue[w] <- residue
    rm(w,excess,residue,data,baseline,yPrev,nextTime,tStart)
    beat[i,3:4]  = c(seg[1],seg[3])
    beat[i, 5] <- par[1]
    beat[i,6:15] = par
    beat[i,10] = beat[i,10]-beat[i,7]
    beat[i,13] = beat[i,13]-beat[i,7]
    rm(par)
  }
  rm(seg)
  temp <- list(beat, ppg)
  return(temp)
}


FindWithinParams <- function(beats_in, ppg, beat, gs = model2.GetSegment, fp = model2.FixParams3, ms = simplex.MakeSimplex3, m2 = model2.ChiSq3, beat_vector = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time, w){
  a <- list()
  for(i in 1:beats_in){         
    
    par <- as.numeric(beat[i,5:16])
    #par <- fp(data[, 1:2], par, rp = renal_param, sys_t = sys_t)      # Do we need to fix the parameters here...?
    beat_indi <- list(1, beat_vector[[2]][i], beat_vector[[3]][i])
                                                                                    
    a[[i]] <- ms(ppg = ppg, param = par, f = m2, inScale = 0.1, inTol=-1, beat_vector = beat_indi, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time[i], w = w[i]) 
  }
  return(a)
}

make_matrix <- function(sim, a){
  # Save the top row of sim for replication:
  top_row_sim <- sim[1, c(5:6, 8:9, 11:12)]              
  #Remove redundant rows and columns from sim:
  sim <- sim[c(6:7, 9:10, 12:13), c(5:6, 8:9, 11:12)]                           
  # You need the top_row of sim to replicate:
  top_row_sim <- matrix(data = top_row_sim, nrow = 6, ncol = 6, byrow = TRUE)    
  
  # Make the a values just the rows where within beat parameters are changed  
  # Save the top row of each matrix of a for replication...
  top_row <- list()
  for(i in 1:beats_in){                                                          
    top_row[[i]] <- a[[i]][1, -c(5:6, 8:9, 11:12)]
    a[[i]] <- a[[i]][-c(1, 6:7, 9:10, 12:13), -c(5:6, 8:9, 11:12)]             
  }
  # Top row needs to be replicated for each within beat row when the across-beat params are being changed:
  for(i in 1:beats_in){
    top_row[[i]] <- matrix(data = top_row[[i]], ncol = 6, nrow = 6, byrow = TRUE)  
  }
  
  
  # Assemble Matrix:

  # Bind replicate rows of top_row for each beat to sim:              
  for(i in 1:beats_in){
    sim <- cbind(sim, top_row[[i]])
  }
  
  
  # Create rows for each beat
  beat_rows <- list()                                                 
  for(i in 1:beats_in){                                             
    
    # Add values to the left
    beat_rows[[i]] <- top_row_sim   
    if(i != 1){
      for(j in 1:(i-1)){
        beat_rows[[i]] <- cbind(beat_rows[[i]], top_row[[j]])  
      }
    }
    
    # Add A
    beat_rows[[i]] <- cbind( beat_rows[[i]], a[[i]])
    
    # Add values to the right
    if(i == beats_in){
      break
    }else{
      for(j in (i+1):beats_in){
        beat_rows[[i]] <- cbind(beat_rows[[i]], top_row[[j]])
      }
    }
  }
  
  # Bind rows together:
  for(i in 1:beats_in){
    sim <- rbind(sim, beat_rows[[i]])
  }
  
  # Add the top row:
  final_top_row <- top_row_sim[1, ]
  for(i in 1:beats_in){
    final_top_row <- c(final_top_row, top_row[[i]][1, ])
  }
  sim <- rbind(final_top_row, sim)
  
  return(sim)
}


extractOutput <- function(beats_in, sim){
  across <- sim[1, ][1:6]
  within <- list()
  for(i in 1:beats_in){
    temp <- rep(0, 12)
    temp[c(1:4, 7, 10)] <-  sim[1, ][((i*6)+1):((i*6)+6)]  
    within[[i]] <- temp
  }
  temp <- list(across, within)
  return(temp)
}


FixOutput <- function(beats_in, beat, ppg, gs = model2.GetSegment, fp = model2.FixParams3, across = output[1], within = output[2], sys_time = sys_time){
  fixed <- list()
  for(i in 1:beats_in){
    seg <- c(beat[i,3],0,beat[i,4])
    data <- model2.GetSegment(ppg,seg)
    rm(seg)
    fixed[[i]] <- model2.FixParams3(data, params = as.numeric(within[[i]]), across_beat_params = across, sys_t = sys_time[i])
  } 
  return(fixed)
}


UpdateBeat <- function(beats_in, beat, fixed){
  new_beat <- data.frame(matrix(0, ncol = 12, nrow = beats_in))
  for(i in 1:beats_in){
    new_beat[i, ] <- fixed[[i]]
  }
  new_beat <- cbind(beat[, 1:4], new_beat)
  return(new_beat)
}


FixBaseline <- function(new_beat, f = model2.ChiSq3, renal_param, dias_param, sys_time, w){
  for(j in 1:nrow(new_beat)){
    if(abs(new_beat[j, 6] - new_beat[j, 5]) < 5){
      # Assess fit:
      wave_check <- model2.ChiSq3(data = ppg, params = as.numeric(new_beat[j, 5:16]), beats = list(1, new_beat[j, 3], new_beat[j, 4]), beat = NULL, a = NULL, plot = FALSE, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time[j], w = w[j])
      # Assess fit with baselines equal:
      wave_check2 <- model2.ChiSq3(data = ppg, params = c(rep(new_beat[j, 5], 2), as.numeric(new_beat[j, 7:16])), beats = list(1, new_beat[j, 3], new_beat[j, 4]), beat = NULL, a = NULL, plot = FALSE, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time[j], w = w[j])
      # If baselines equal gives a better value of ChiSq, fix them to be so:
      if(wave_check2 < wave_check){
        new_beat[j, 6] <- new_beat[j, 5]
      }
    }
  }
  return(new_beat)
}


PlotFits <- function(beats_in, ppg, beat2, gs = model2.GetSegment, rb = model2.Rebuild2){
  for(i in 1:beats_in){
    seg <- c(beat[i,3],0,beat[i,4])  
    data <- model2.GetSegment(ppg,seg)
    yPrev <- ppg[seg[1]-1,2]
    xPrev <- ppg[seg[1]-1, 1]
    xNext <- ppg[seg[3], 1]
    rm(seg)
    temp<-model2.Rebuild2(data, yPrev, as.double(beat2[i,]),TRUE)    
    plot(data[, 1], data[, 2], ylim = c(beat2$Baseline[1]*1.5, max(data[, 2]*1.2)), main = paste(c("batch", k, "wave", i), collapse = " "))   # ylim = c(76, 86)
    lines(data[,1],temp)
    # Plot baselines:
    lines(c(xPrev, (beat2[i, 3]  + (1*beat2[i, 6]))), rep(beat2[i, 1], 2))   
    lines(c((beat2[i, 3]  + (1*beat2[i, 6])), xNext), rep(beat2[i, 2], 2))
    # Plot systolic:
    # Always need 12 elements, but set amplitude and width to 0 for the peaks that aren't being used. 
    par <- as.double(beat2[i,])
    par[c(7:8, 10:11)] <- 0
    temp<-model2.Rebuild2(data,yPrev,par,TRUE)
    lines(data[,1],temp, col = "red")
    # Plot diastolic:
    par <- as.double(beat2[i,])
    par[c(4:5, 10:11)] <- 0
    temp<-model2.Rebuild2(data,yPrev,par,TRUE)      
    lines(data[,1],temp, col = "blue")
    # Plot renal:
    par <- as.double(beat2[i,])
    par[c(4:5, 7:8)] <- 0
    temp<-model2.Rebuild2(data,yPrev,par,TRUE)
    lines(data[,1],temp, col = "green")
  }
}

GGplotFits <- function(beats_in, ppg, beat2, gs = model2.GetSegment, rb = model2.Rebuild2, run, pr, p = F){
  
  if(pr == 1){pr = 0}
  
  for(i in 1:beats_in){
    seg <- c(beat[i,3],0,beat[i,4])  
    data <- model2.GetSegment(ppg,seg)
    yPrev <- ppg[seg[1]-1,2]
    xPrev <- ppg[seg[1]-1, 1]
    xNext <- ppg[seg[3], 1]
    rm(seg)
    temp<-model2.Rebuild2(data, yPrev, as.double(beat2[i,]),TRUE)    
    fit <- model2.Rebuild2(data, yPrev, as.double(beat2[i,]),TRUE) 
    
    # Create waves dataframe:
    waves <- data.frame(data)
    waves <- cbind(data, fit)
    par <- as.double(beat2[i,])
    par[c(7:8, 10:11)] <- 0
    temp<-model2.Rebuild2(data,yPrev,par,TRUE)
    waves <- cbind(waves, temp)
    par <- as.double(beat2[i,])
    par[c(4:5, 7:8)] <- 0
    temp<-model2.Rebuild2(data,yPrev,par,TRUE)
    waves <- cbind(waves, temp)
    par <- as.double(beat2[i,])
    par[c(4:5, 10:11)] <- 0
    temp<-model2.Rebuild2(data,yPrev,par,TRUE)      
    waves <- cbind(waves, temp)

    # Baselines can be added as line segments with geom_segment:
    b1y <- beat2[i, 1]
    b1x <-  c(xPrev, (beat2[i, 3]  + (1*beat2[i, 6])))  
    b2x <- c((beat2[i, 3]  + (1*beat2[i, 6])), xNext)
    b2y <-  beat2[i, 2]
    
    # Make component waves + fit into splines:
    sfunction <- splinefun(1:nrow(waves), fit, method = "natural")
    fit2 <- sfunction(seq(1, nrow(waves), 0.1), deriv = 0) 
    par <- as.double(beat2[i,]) 
    par[c(7:8, 10:11)] <- 0
    sys<-model2.Rebuild2(data,yPrev,par,TRUE)
    sfunction <- splinefun(1:nrow(waves), sys, method = "natural")
    sys2 <- sfunction(seq(1, nrow(waves), 0.1), deriv = 0) 
    par <- as.double(beat2[i,])
    par[c(4:5, 10:11)] <- 0
    dias<-model2.Rebuild2(data,yPrev,par,TRUE)      
    sfunction <- splinefun(1:nrow(waves), dias, method = "natural")
    dias2 <- sfunction(seq(1, nrow(waves), 0.1), deriv = 0) 
    par <- as.double(beat2[i,])
    par[c(4:5, 7:8)] <- 0
    R1<-model2.Rebuild2(data,yPrev,par,TRUE)
    sfunction <- splinefun(1:nrow(waves), R1, method = "natural")
    R12 <- sfunction(seq(1, nrow(waves), 0.1), deriv = 0) 

    # Build waves_stacked 
    time <- waves[, 1]
    time <- seq(from = time[1], to = time[length(time)], length.out = length(fit2))
    fit2 <- data.frame(time, fit2)
    fit2 <- cbind(fit2, rep("fit", nrow(fit2)))
    sys2 <- data.frame(time, sys2)
    sys2 <- cbind(sys2, rep("systolic wave", nrow(sys2)))
    dias2 <- data.frame(time, dias2)
    dias2 <- cbind(dias2, rep("2nd reflectance wave", nrow(dias2)))
    R12 <- data.frame(time, R12)
    R12 <- cbind(R12, rep("1st reflectance wave", nrow(R12)))
    colnames(fit2) <- c("x", "values", "Wave")
    colnames(sys2) <-  c("x", "values", "Wave")
    colnames(dias2) <-  c("x", "values", "Wave")
    colnames(R12) <-  c("x", "values", "Wave")
    waves_stacked_final <- rbind(fit2, sys2, dias2, R12)
    
    # stack data:
    data_stacked <- data.frame(data)
    data_stacked <- cbind(data_stacked, rep("data"))
    colnames(data_stacked) <- c("x", "values", "Wave")
    
    # Plot! 
    if(p == T){
      c <- ggplot(data = waves_stacked_final, aes(x = x, y = values, col = Wave)) + geom_line(aes(size = Wave, alpha = Wave)) +
              scale_color_manual(values = c("#03fc7b", "#03b5fc", "black", "black", "black", "#ff4242", "black")) + scale_size_manual(values = c(0.7, 0.7, 1.5, 0.7, 0.7)) + 
              scale_alpha_manual(values = c(1, 1, 1, 1, 1)) + ylab("PPG Signal") + xlab("Time") + geom_point(data = data_stacked) + 
              geom_segment(aes(x = data[1, 1], y = b1y, xend = b1x[2], yend = b1y, colour = "black")) + geom_segment(aes(x = b2x[1], y = b2y, xend = b2x[2], yend = b2y, colour = "black")) + 
              theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
              theme(legend.position = "none") + ggtitle(paste0("Participant", " ", run, "\n", pr, " ", "mcg", sep = "")) # + ylim(-350, 1600) 
    }else{
        ggplot(data = waves_stacked_final, aes(x = x, y = values, col = Wave)) + geom_line(aes(size = Wave, alpha = Wave)) +
        scale_color_manual(values = c("green", "blue", "black", "black", "black", "red", "black")) + scale_size_manual(values = c(0.7, 0.7, 1.5, 0.7, 0.7)) + 
        scale_alpha_manual(values = c(1, 1, 1, 1, 1)) + ylab("PPG Signal") + xlab("Time") + geom_point(data = data_stacked) + 
        geom_segment(aes(x = b1x[1], y = b1y, xend = b1x[2], yend = b1y, colour = "black")) + geom_segment(aes(x = b2x[1], y = b2y, xend = b2x[2], yend = b2y, colour = "black")) + 
        theme(legend.position = "none") # + ylim(-1000, 2000)
    }
  }  
}



osnd_fit <- function(bf = beat_final, ppg, gs = model2.GetSegment, r = model2.Rebuild2, sf = splinefun, dp = diast_pk, oa = osnd_of_average, sr = samplingRate, plot = FALSE){
  
  osnd_diff <- list()
  for(i in 1:nrow(bf)   ){  #nrow(bf)  
    # Find the correct data segments and corresponding model fit:
    seg <- c(bf[i,3],0,bf[i,4])  
    data <- gs(ppg,seg)
    yPrev <- ppg[seg[1]-1,2]
    xPrev <- ppg[seg[1]-1, 1]
    xNext <- ppg[seg[3], 1]
    rm(seg)
    temp <- r(data, yPrev, as.double(bf[i,-c(1:4)]),TRUE)   
    
    # Upsample fit - so that fit OSND can be calculated as the data would have been in the main script
    sfunction <- sf(1:length(temp), temp, method = "natural")
    fit <-  sfunction(seq(1, length(temp), 0.1), deriv = 0)
    # Upsample data segment:
    sfunction <- sf(1:length(data[, 2]), data[, 2], method = "natural")
    dat <-  sfunction(seq(1, length(data[, 2]), 0.1), deriv = 0)
    
    # plot(dat)
    # plot(fit)
    
    # Find OSND of fit:
    tmp <- dp(avw = fit, sr = sr, scale = T, dias_param = bf[i, 10]*sr*10)      # pass in the Dparam to help find D on fit waves
    dPeak <- tmp[1]
    xShift <- tmp[2]
    rm(tmp)
    osnd_fit <- oa(fit, dp = dPeak, diff = 0, sr = sr, plot = F)
    # Find OSND of data:
    tmp <- dp(avw = dat, sr = sr, scale = T, dias_param = bf[i, 10]*sr*10)  # why not make dias_param also informed by the model here? instead of null? try it!
    dPeak <- tmp[1]
    xShift <- tmp[2]
    rm(tmp)
    osnd_dat <- oa(dat, dp = dPeak, diff = 0, sr = sr, plot = F)
    
    # Adjust x-axis for upsampling and sampling rate, then find the difference between OSND points:
    osnd_dat$x <- osnd_dat$x/(10*sr)
    osnd_fit$x <- osnd_fit$x/(10*sr)
    osnd_diff[[i]] <- osnd_dat - osnd_fit
    
    if(plot == TRUE){
      # Plot them together:
      plot((1:length(dat))/(10*samplingRate), dat, type = "l", xlab = "time", ylab = "")
      lines((1:length(dat))/(10*samplingRate), fit, col = "red")
      points(osnd_dat, pch = 19)
      points(osnd_fit, col = "red", pch = 19)
    }
  }
  return(osnd_diff)
}


ArrangeOutputs <- function(beat_final, beat_orig, features, pulse, fit_check, ps, pr){
  
  # Rename rows
  rownames(beat_final) <- colnames(pulse)[-1][1:nrow(beat_final)]
  rownames(features) <- colnames(pulse)[-1]
  
  # Reorganize fit_check:
  # Individual wave fits:
  wave_fits <- c()
  for(i in 1:length(fit_check)){
    wave_fits <- c(wave_fits, as.numeric(fit_check[[i]][[2]]))
  }
  # Maximum error:
  max_err <- c()
  for(i in 1:length(fit_check)){
    max_err <- c(max_err, as.numeric(fit_check[[i]][[3]]))
  }
  # NRMSE:
  NRMSE <- c()
  for(i in 1:length(fit_check)){
    NRMSE <- c(NRMSE, as.numeric(fit_check[[i]][[4]]))
  }
  # aNRMSE:
  aNRMSE <- c()
  for(i in 1:length(fit_check)){
    aNRMSE <- c(aNRMSE, as.numeric(fit_check[[i]][[5]]))
  }
  fit_check <- list(wave_fits, max_err, NRMSE, aNRMSE)
  
  tmp <- list(beat_final, features, fit_check)
  return(tmp)
}


model2.GetSegment <- function(ppg,limits){
  
  w <- c(limits[1]:limits[3])
  result <- matrix(nrow=length(w),ncol=2)
  result[,1] <- ppg[[lab.time]][w]
  result[,2] <- ppg[[lab.ppg]][w]
  
  return(result)
}

model2.Excess <- function(y,offset,baseline){
  count <- length(y)
  if (length(offset) == 0){
    print("Help")
  }
  
  result <- 1:count * 0.0
  result[1] = y[1] - (baseline + config.rate*(offset-baseline))
  for (j in 2:count){
    result[j] = y[j] - (baseline + config.rate*(y[j-1]-baseline))
  }
  
  return(result)
}

model2.Peak <- function(time,peakParams){
  temp <- 2*const.pi*(time - as.double(peakParams[1]))/as.double(peakParams[3])
  temp[which(temp < -const.pi)] = -const.pi
  temp[which(temp >  const.pi)] =  const.pi
  result <- as.double(peakParams[2]) * (0.5 * (1+cos(temp)))^2
  
  return(result)
}

model2.SubtractExcessPeak <- function(time,residue,peakParams){
  result <- residue - model2.Peak(time,peakParams)
  
  return(result)
}


model2.ChiSq3 <- function(data, params, debug=FALSE, beats, optional = NULL, beat = NULL, a = NULL, plot = FALSE, renal_param, dias_param, sys_time, w){  
  
  # makesimplex3 inputs a single set of parameters (tParam) to test, whereas run.simplex2 inputs a matrix
  # This determines where within beat parameters should be extracted from, hence beat and a are NULL unless otherwise specified. 
  
  # Across-beat parameter extraction:
  if(!is.null(a)){                                            # If a 66 parameter vector has been supplied, extract the first 6 
    across_beat_params <- a[1:6]
  }else{                                                      # If not, take them from the params input
    par <- params
    across_beat_params <- par[c(5, 6, 8, 9, 11, 12)]
  }
  
  # Calculation of ChiSq for all beats:
  beat_fit <- list()
  for(i in 1:beats[[1]]){                                          # The number of beats is determined by the first object of beats
    
    # Within-beat parameter extraction:
    if(!is.null(a)){                                               # If a 66 parameter vector has been supplied, take those values
      par2 <- a[((i*6)+1):((i*6)+6)]    
    }else{                                   
      if(!is.null(beat)){                                           # If not, take the values of beat. 
        par2 <- as.numeric(beat[i, c(5:8, 11, 14)])
      }else{                                                        # If beat is also not provided, take them from the params input
        par2 <- par[c(1:4, 7, 10)]
      }                                                            
    }
    
    # Extract individual beat data:  
    seg <- c(beats[[2]][i],0,beats[[3]][i])
    dat <- model2.GetSegment(data,seg)
    rm(seg)
    
    # Extract systolic and diastolic parameters:
    sys <- par2[3]
    #dias <- across_beat_params[2] + par2[3]
    dias <- par2[3] + dias_param
    end <- which(abs(dat[, 1]-dias) == min(abs(dat[, 1] - dias)))
    
    # Find W:
    w. <- w[i]
    w. <- which(abs(dat[, 1] - w.) == min(abs(dat[, 1] - w.))) 
  
    sys_t <- sys_time[i]
    start <- which(abs(dat[, 1]-sys_t) == min(abs(dat[, 1] - sys_t))) 
    
    # Fix parameters and calculate penalty:
    temp <- model2.FIX_PAR3(time = dat[, 1], within_beat_params = par2, across_beat_params = across_beat_params, renal_param = renal_param, sys_t = sys_t)  
    penalty <- temp[1]
    fixedPar <- temp[2:length(temp)]     
    rm(temp)
    
    # Calculate fit and residue:
    fit <- model2.Rebuild2(dat,dat[1,2],params = fixedPar)    
    residue <- dat[ ,2] - fit
    
    # Add to the penalty if the residual at sys_t is high:
    if(residue[start]*residue[start] > 0){
      penalty <- penalty + residue[start]*residue[start]
    }
    
    # Weighted region is W -> D (with slope)
    residue[w.:end[1]] <-  residue[w.:end[1]]*3
    if(length(residue) > end[1]){
      tail <- (end[1]+1):length(residue)
      for(j in 1:length(tail)){
        wgt <- 3 - (0.1*j)
        if(wgt < 1){wgt <- 1}
        residue[tail[j]] <- residue[tail[j]]*wgt
      }
    }
    
    # Calculate Reduced Chi-Square for the beat:
    nData <- nrow(dat)    
    nPar <- length(par2) 
    beat_fit[[i]] <- (sum(residue*residue) / (nData-nPar)) + as.numeric(penalty)
    
    if(plot == TRUE){
      plot(dat,  ylim = c(-150, 2000))      #ylim = c(76, 86) for bioradio data, ylim = c(-150, 1600) for ISO
      lines(dat[, 1], fit)
      #lines(dat[, 1], residue + dat[1, 2])
    }
  }
  
  # Summate individual beat ChiSq values:
  temp <- c()
  for(i in 1:length(beat_fit)){
    temp[i] <- beat_fit[[i]][1]
  }
  ts_fit <- sum(temp)
  return(ts_fit)
}


model2.ChiSq4 <- function(data, params,debug=FALSE, beats, beat, a = NULL, plot = FALSE, renal_param, dias_param, sys_time, w){  
  
  # Across-beat parameter extraction:
  if(!is.null(a)){                                            # If a 66 parameter vector has been supplied, extract the first 6 
    across_beat_params <- a[1:6]
  }else{                                                      # If not, take them from the params input
    par <- params
    across_beat_params <- par[c(5, 6, 8, 9, 11, 12)]
  }
  
  # Calculation of ChiSq for all beats:
  beat_fit <- c()   
  max_error <- c()   
  NRMSE <- c()
  aNRMSE <- c()
  for(i in 1:beats[[1]]){                                          # The number of beats is determined by the first object of beats
    
    # Within-beat parameter extraction:
    if(!is.null(a)){                                               # If a 66 parameter vector has been supplied, take those values
      par2 <- a[((i*6)+1):((i*6)+6)]    
    }else{                                   
      if(!is.null(beat)){                                           # If not, take the values of beat. 
        par2 <- as.numeric(beat[i, c(5:8, 11, 14)])
      }else{                                                        # If beat is also not provided, take them from the params input
        par2 <- par[c(1:4, 7, 10)]
      }                                                            
    }
    
    # Extract individual beat data:  
    seg <- c(beats[[2]][i],0,beats[[3]][i])
    dat <- model2.GetSegment(data,seg)
    rm(seg)
    
    # Extract systolic and diastolic parameters:
    sys <- par2[3]
    #dias <- across_beat_params[2] + par2[3]
    dias <- par2[3] + dias_param
    start <- which(abs(dat[, 1]-sys) == min(abs(dat[, 1] - sys)))   
    end <- which(abs(dat[, 1]-dias) == min(abs(dat[, 1] - dias)))
    
    # Find W:
    w. <- w[i]
    w. <- which(abs(dat[, 1] - w.) == min(abs(dat[, 1] - w.))) 
    
    # Get intially estimated systolic timing and amplitude:
    sys_t <- sys_time[i]
    
    # Fix parameters and calculate penalty:
    temp <- model2.FIX_PAR3(time = dat[, 1], within_beat_params = par2, across_beat_params = across_beat_params, renal_param = renal_param, sys_t = sys_t)  
    penalty <- temp[1]
    fixedPar <- temp[2:length(temp)]     
    rm(temp)
    
    # Calculate fit, residue and max error:
    fit <- model2.Rebuild2(dat,dat[1,2],params = fixedPar)    
    residue <- dat[ ,2] - fit
    max_error[i] <- max(residue)
    
    # Before weighting the residuals, calculate NMRSE for the region we are interested in:
    if(plot == TRUE){
      plot(dat, ylim = c(-500, 2500))
      lines(dat[, 1], fit)
      lines(dat[, 1], residue, col = "Red") 
    }
    
    # Define region of interest:
    rmse_begin <- floor((1+w.)/2)    # Begin half way from O to W
    rmse_end <- end[1] + 10          # End 10 points after the d-peak (this corresponds to half way down the weighted tail)
    if(sum(is.na(residue[rmse_begin:rmse_end])) > 0){
      residue_roi <- residue[rmse_begin:length(residue)]    # If there are fewer than 10 data points after D, use as many as there are
      ind_resid <- rmse_begin:length(residue)
    }else{
      residue_roi <- residue[rmse_begin:rmse_end]
      ind_resid <- rmse_begin:rmse_end
    }
    if(plot == TRUE){lines(dat[ind_resid, 1],residue_roi, col = "green")} 
  
    # Define null model:
    fit_null <- rep(mean(dat[ind_resid, 2], trim = 0), length(residue_roi))
    if(plot == TRUE){lines(dat[ind_resid, 1], fit_null)}
    # Calculate residuals of the null model:
    residuals_of_null_model <- dat[ind_resid, 2] - fit_null
    
    # Calculate NRMSE:
    rmse_model2 <-  sqrt(mean(residue_roi^2, trim = 0))
    rmse_null <- sqrt(mean(residuals_of_null_model^2))
    NRMSE. <- 1 - (rmse_model2 / rmse_null)
    NRMSE[i] <- NRMSE.
    
    # Alternative NRMSE method (Wang et al 2013):
    # SSE / Sum of squared datapoints 
    aNRMSE[i] <- (sum(residue_roi^2) / sum(dat[ind_resid, 2]^2))*100 
    #plot(dat[ind_resid, 1], dat[ind_resid, 2]^2, type = "l")
    #lines(dat[ind_resid, 1], residue_roi^2, col = "red")
    
    # Weighted region is W -> D (with slope)
    residue[w.:end[1]] <-  residue[w.:end[1]]*3
    if(length(residue) > end[1]){
      tail <- (end[1]+1):length(residue)
      for(j in 1:length(tail)){
        wgt <- 3 - (0.1*j)
        if(wgt < 1){wgt <- 1}
        residue[tail[j]] <- residue[tail[j]]*wgt
      }
    }
    
    # Calculate Reduced Chi-Square for the beat:
    nData <- nrow(dat)    
    nPar <- length(par2) 
    if(par2[1] == par2[2]){    # If baselines are the same, consider them as 1 parameter
      nPar <- nPar - 1
    }
    beat_fit[i] <- (sum(residue*residue) / (nData-nPar)) + as.numeric(penalty)
    
  }
  
  # Summate individual beat ChiSq values:
  temp <- c()
  for(i in 1:length(beat_fit)){
    temp[i] <- beat_fit[i]
  }
  ts_fit <- sum(temp)
  rm(temp)
  
  fit <- list(ts_fit, beat_fit, max_error, NRMSE, aNRMSE)
  return(fit)
}


model2.Rebuild2 <- function(xy,offset,params,invert=TRUE){     
  result <- 1:nrow(xy) * 0.0
  # Creating the excess:
  result <- result + model2.Peak(xy[,1],params[3:5])     # Systolic parameters 
  if (length(params)>=8){
    result <- result + model2.Peak(xy[,1],params[6:8]+c(params[3],0,0))   # Diastolic parameters 
  }
  if (length(params)>=11){
    result <- result + model2.Peak(xy[,1],params[9:11]+c(params[3],0,0))  # Renal parameters
  }
  # Adding decay (config.rate + baseline parameters):
  if (invert){
    result <- model2.Excess.Inv2(xy[,1],result,offset,params[1],params[2],params[3]+1*params[6], config.rate = params[12])   
  }
  return(as.double(result))
}


model2.Excess.Inv2 <- function(time,excess,offset,baselineStart,baselineEnd,timeBase,config.rate){   
  nX <- length(excess)
  if (nX == 0){
    print("Help")
  }
  result <- 1:nX * 0.0
  baseline <- time * 0 + baselineStart
  baseline[which(time > timeBase)] = baselineEnd
  
  # If excess has NAs it will interrupt the reconstruction, remove them:
  temp <- which(is.nan(excess))
  if(length(temp) > 0){
    for(i in 1:length(temp)){
      excess[temp][i] <- 0 
    }
  }
  
  # Adding the decay element to the excess (one value at a time):
  result[1] = excess[1] + (baselineStart + config.rate*(offset-baselineStart)) 
  for (j in 2:nX){  
    result[j] = excess[j] + (baseline[j] + config.rate*(result[j-1]-baseline[j]))  
  }
  return(result)
}


model2.FIX_PAR3 <- function(time, within_beat_params, across_beat_params, debug=FALSE, renal_param, sys_t){
  
  # params: {Baseline, {baseline 2}, t_sys, H_sys, W_sys, {dt_1, H_1, W_1, {dt_2, H_2, W_2}}}
  # across_beat_params: { w[1], t[2], w[2], t[3], w[3] }
  
  nPar <- length(within_beat_params) + length(across_beat_params)
  nData <- length(time)
  
  # Transcribe parameters
  nBase <- 1
  baseline <- c( within_beat_params[1], within_beat_params[1] )
  if (nPar == 6 | nPar == 9 | nPar == 12){     
    baseline[2] = within_beat_params[2]
    nBase <- 2
  }
  
  # par: {base1, {base2}, t[1], h[1], #, #, { h[2], #, #, { h[3], ..., ... }}}
  
  t <- c( within_beat_params[nBase + 1], across_beat_params[2], across_beat_params[4])           # time (systolic = within, diastolic / renal = across)
  h <- c( within_beat_params[nBase + 2], 0, 0 )                                                  # height (systolic = within, diastolic / renal default to 0 unless peaks supplied (see below))
  w <- c( across_beat_params[1], across_beat_params[3], across_beat_params[5])    # width (systolic / diastolic / renal = across)
  hasPeak <- c( TRUE, FALSE, FALSE )
  
  # Calculate penalty only if a peak has been supplied       
  
  if (nPar >= nBase + 7){                                    # Assign diastolic values if peak present
    hasPeak[2] = TRUE
    t[2] <- across_beat_params[2]
    h[2] <- within_beat_params[nBase + 3]
    w[2] <- across_beat_params[3]
  }
  
  if (nPar >= nBase + 10){                                   # Assign renal values if peak present  
    hasPeak[3] = TRUE
    t[3] <- across_beat_params[4]
    h[3] <- within_beat_params[nBase + 4]
    w[3] <- across_beat_params[5]
  }
  
  # Clamp and/or penalize parameters
  penalty <- 0
  
  # 
  tMin <- time[1]   
  tMax <- time[length(time)]       
  
  META_BASELINE_SHIFT <- 1.0    # penalty for how big the gap is between the two baselines
  META_MIN_PEAK_DELAY <- 0.1    # peaks cannot be following one another by less than 0.1ms
  MIN_WIDTH <- c(0.05, 0.05, 0.1) 
  MAX_WIDTH <- c(0.5, 0.45, 0.25)
  
  p <- 1:12*0    # One penalty value for each parameter
  # p: { #, #, t[1], h[1], w[1], t[2], h[2], w[2], t[3], h[3], w[3], across_beat_params[6] }
  
  # Fix height and width for each of the three waves (1:3)      
  for (i in 1:3){
    if (h[i] < 0){                                           # Heights should not be negative
      penalty <- penalty + h[i]*h[i] 
      p[3*i+1] <- h[i]*h[i]                
      h[i] <- 0
    }
    
    if (w[i] < MIN_WIDTH[i] | w[i] > MAX_WIDTH[i]){           # Correct widths as per MIN/MAX_WIDTH
      fixed <- max(MIN_WIDTH[i], min( w[i], MAX_WIDTH[i]))             
      diff <- fixed - w[i]
      penalty <- penalty + diff*diff
      p[3*i+2] <- diff*diff            
      w[i] <- fixed
    }
    
    if(i==3){                                           # Renal peak should be penalized as its amplitude increases
      if( h[3] > (h[1]/50)){                            # As soon as it's amplitude exceeds 2% of the systolic amp. 
        diff <- h[3] - (h[1]/50)
        penalty <- penalty + 2*diff*diff
        p[10] <- p[10] + 2*diff*diff
      }
    }
  }

  # Fix time
  
  # Systolic:
  fixed <- max((sys_t - 0.04) , min( t[1], (sys_t + 0.04 )))      # Making sure S peak sits within 40ms of the the peak of the data
  if (debug){
    print(paste("time S: ",tMin," < ",t[1]," < min( ",tMin+1,",",tMax," )"))
  }
  if (t[1] != fixed){
    diff <- fixed - t[1]
    penalty <- penalty + 10^8*diff*diff   
    p[3] <- 10^8*diff*diff
    t[1] <- fixed
  }
  
  # Diastolic:
  fixed <- max( 2 * META_MIN_PEAK_DELAY, min( t[2], tMax - tMin + 0.4 * w[2] ) )   # This stops diastolic time being < 0.2
  if (debug){
    print(paste("time D: ",2 * META_MIN_PEAK_DELAY," < ",t[2]," < ",tMax - tMin + 0.4 * w[2]," )"))
  }
  if (t[2] != fixed){
    # Two peak delays between S and D
    diff <- fixed - t[2]
    if (hasPeak[2]){
      penalty <- penalty + diff*diff  
      p[6] <- diff*diff
    }
    t[2] <- fixed
  }
  
  # Renal:
  fixed <- max( max(META_MIN_PEAK_DELAY, renal_param - 0.02), min( t[3], t[2] - META_MIN_PEAK_DELAY, renal_param + 0.02 ) )   # Stops renal peak being < 0.1 after systolic or < 0.1 before diastolic, and within 20ms of renal param
  if (debug){
    print(paste("time R: ",META_MIN_PEAK_DELAY," < ",t[3]," < ",t[2] - META_MIN_PEAK_DELAY," )"))
  }
  if (t[3] != fixed){
    diff <- fixed - t[3]
    if (hasPeak[3]){
      penalty <- penalty + 5*10^7*diff*diff 
      p[9] <- diff*diff
    }
    t[3] <- renal_param
  }

  # Config.rate
  if(across_beat_params[6] > 0.95){
    diff <- across_beat_params[6] - 0.95
    penalty <- penalty + 10^7*diff*diff
    p[12] <- 10^7*diff*diff
    across_beat_params[6] <- 0.95
  }
  
  # Baseline1 shouldn't be above 0:
  if(baseline[1] > 0){
    penalty <- penalty + baseline[1]*baseline[1]
    p[1] <- baseline[1]*baseline[1]
    baseline[1] <- 0
  }

  fixedPar <- c( baseline[1:2], t[1], h[1], w[1], t[2], h[2], w[2], t[3], h[3], w[3], across_beat_params[6])
  
  if (debug){
    print(p)
  }
  
  return( c( penalty, fixedPar ) )
}


model2.FixParams3 <- function(data,params, across_beat_params = NULL, debug=FALSE, rp = renal_param, sys_t){
  
  # If across_beat_params have not been provided, extract them from params
  if(is.null(across_beat_params)){    
    across_beat_params <- params[c(5, 6, 8, 9, 11, 12)]
  }
  
  temp <- model2.FIX_PAR3(time = data[, 1], within_beat_params = params[c(1:4, 7, 10)], across_beat_params, debug = F, renal_param = rp, sys_t)  
  return( temp[2:length(temp)] )     # first value of temp is penalty
} 



# Make Simplex 2 (for across beat parameters only):
simplex.MakeSimplex2 <- function(data,param,f,inScale,directions=NULL,inTol=-1, optional=NULL,debug=FALSE, beat_vector = beat_vector,
                                 beat = beat, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w){
  ########################################################################################################################################
  # MakeSimplex2 iterates on the across-beat starting parameters, refining them to give to give the simplex a good starting position. 
  
  # Inputs: 
  # data ()
  # param
  # f
  # inScale
  # directions
  # inTol
  # optional
  # debug
  # beat_vector
  # beat
  # renal_param
  # dias_param
  # sys_time
  # w
  
  
  # Outputs:
  # ?? ()
  ########################################################################################################################################
  
  if(debug){print("MakeSimplex -- debug")}                # this seems unecessary now..         
  
  nPar <- length(param)                  # define number of parameters
  nScale <- length(inScale)               # what is inscale?
  
  
  if (nScale == 0)       # Not sure what this is doing.. 
  {
    scale <- 1:nPar * 0 + 1
  } else if (nScale == 1){
    scale <- 1:nPar * 0 + inScale
  } else if (length(inScale) == nPar){
    scale <- inScale
  } else {
    #print("Invalid scale vector length")
    return("Error: Invalid scale vector length")
  }
  
  
  if (length(inTol) == 1 & inTol > 0){   # If no tolerance is provided, the tol is defined as the value of the point / vertex provided
    tol <- inTol[1]
  } else {
    tol <- min(1,f(data,param, beats = beat_vector, beat = beat, renal_param = renal_param,
                   dias_param = dias_param, sys_time = sys_time, w = w))  
  }
  
  
  chiSq <- 1:(nPar+1) * 0.0    # Creating a vector of ChiSq values... 
  
  chiSq[1] <- f(data,param,optional=optional, beats = beat_vector, beat = beat, renal_param = renal_param,    # ChiSq[1 is the fit when no parameters are changed... 
                dias_param = dias_param, sys_time = sys_time, w = w)                                            
  
  if (debug){ print(paste("Root chi-squared:",chiSq[1]))}   # is this line still necessary?
  
  result <- matrix(nrow=nPar+1,ncol=nPar)      # Create a matrix of parameters, with as many columns as parameters, and nPar + 1 rows (each will be one point of the simplex)
  result[1,] <- as.double(param)    # Fill the first row with the inputted parameters (our best guess so far)... 
  
  useDirections = !is.null(directions)         # I don't think we ever do use directions...?
  
  if (useDirections){ useDirections <- nrow(directions) == nPar & ncol(directions) == nPar}     # ditto, we are not using directions.. but is it necessary?
  
  
  for (i in c(5, 6, 8, 9, 11, 12)){                  # Create a for loop for each of the across beat parameters (the ones we are refining)
    
    if (debug){ print(paste("Parameter",i)) }  # We are not debugging now so ?necessary
    
    tParam <- param    # create a new vector of parameters to be tweaked / tested, tParam
    
    # Pick a direction
    delta <- 1:nPar * 0              # delta is a finite increment by which to increase or decrease a given parameter value. We create a vector for it here. 
    
    if (useDirections){
      delta <- scale[i] * directions[i,]      # this if is redunant since we are not using directions...
    } else {
      delta[i] <- scale[i]       # scale relates to inScale, somehow.. and is used to set the delta level
    }
    
    tParam <- param - delta                                   # This part tries tweaking each parameter up or down
    
    chiSqMinus <- f(data,tParam,optional=optional, beats = beat_vector, beat = beat, renal_param = renal_param,  # we run the test parameters through chisq3 to get a chisq value for them
                    dias_param = dias_param, sys_time = sys_time, w = w)         
    
    tParam <- param + delta                                   # This time delta is added rather than subtracted, and again the resulting Chisq change is found
    
    chiSq[i+1] <- f(data,tParam,optional=optional, beats = beat_vector, beat = beat, renal_param = renal_param,
                    dias_param = dias_param, sys_time = sys_time, w = w)            
    
    
    if (debug){                       # redundant now? need to go through specifically looking for debug functions and get a sense of what it is doing
      print("Select direction:")
      print(paste("chi^2(",param[i] - delta[i],") =",chiSqMinus))
      print(paste("chi^2(",param[i],") =",chiSq[1]))
      print(paste("chi^2(",param[i] + delta[i],") =",chiSq[i+1]))
      print("---")
    }
    
    if (chiSqMinus < chiSq[i+1]){      # If going down by delta is better than going up by delta, 
      delta <- -delta                  # then replace chiSq[i+1] with the lower score (ChiSqMinus)
      tParam <- param + delta
      chiSq[i+1] <- chiSqMinus
    }
    
    iKill <- 10       # defining the number of iterations that a given parameter will be refined over
    
    if (chiSq[i+1] < chiSq[1]){         # If the new fit is better than the old fit (with no parameters changed), continue to go in the direction that improved the fit
      if (debug){ print("Extending as best point") }
      while (chiSq[i+1] < chiSq[1] + tol){                 # Chisquare keeps getting iterated here (for 10 iterations)
        delta <- 2*delta   
        tParam <- param + delta
        oldScore <- chiSq[i+1]          # The current best fit is called 'old score'
        chiSq[i+1] <- f(data,tParam,optional=optional, beats = beat_vector, beat = beat, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)   # The new fit is now designated ChiSq[i+1]
        if (debug){ print(paste("chi^2(",tParam[i],") =",chiSq[i+1])) }
        if (chiSq[i+1] > oldScore){   # Check if the new fit is worse than current fit
          tParam <- param + 0.5*delta   # If so, make delta what it was one iteration previous (undoing the *2)
          chiSq[i+1] <- oldScore        # and redesignate old score to chiSq[i+1]
          break
        }
        #print(paste(i,"-",delta,":",chiSq[i+1]))
        iKill <- iKill - 1             # If the new fit is not worse than the current fit, knock of one on the interation count,
        if (iKill < 0){                # and keep iterating until either the next fit is worse (and break is called), or iKill ends (and break is also called)
          break
        }
      }
    } else if (chiSq[i+1] < chiSq[1] + tol){    # If the new fit is not better than the old fit, is it at least better than the old fit + tol?
      if (debug){ print("Extending below tolerance") }
      while (chiSq[i+1] < chiSq[1] + tol){
        delta <- 2*delta
        tParam <- param + delta
        oldScore <- chiSq[i+1]
        chiSq[i+1] <- f(data,tParam,optional=optional, beats = beat_vector, beat = beat, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)
        if (debug){ print(paste("chi^2(",tParam[i],") =",chiSq[i+1])) }        
        if (chiSq[i+1] - oldScore < oldScore - chiSq[1]){  
          tParam <- param + 0.5*delta
          chiSq[i+1] <- oldScore
          break
        }
        iKill <- iKill - 1
        if (iKill < 0){
          if(i == 9){
            tParam[9] <- renal_param  # Ignore renal times that can't optomize
            break
          } 
          print(c("simplex constructed as per original parameter"))
          break
          #print("Failed to construct simplex")
          #return(paste("Error: param[",i,"]",sep=""))
        }
      }
    } else {
      if (debug){ print("Shrinking above tolerance") }
      while (chiSq[i+1] > chiSq[1] + tol){              # If the new fit is much worse than the original, reduce the size of delta
        delta <- 0.5*delta
        tParam <- param + delta
        lastChiSq <- chiSq[i+1]
        chiSq[i+1] <- f(data,tParam,optional=optional, beats = beat_vector, beat = beat, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)
        if (debug){ print(paste("chi^2(",tParam[i],") =",chiSq[i+1])) }
        #print(paste(i,"-",delta,":",chiSq[i+1]))
        if (iKill < 0 & (chiSq[i+1]-chiSq[1]) > 0.75 * (lastChiSq-chiSq[1])){
          if(i == 9){
            tParam[9] <- renal_param  # Ignore renal times that can't optomize
            next
          } 
          print(c("simplex constructed as per original parameter"))
          next
          #print("Failed to construct simplex")
          #return(paste("Error: param[",i,"]",sep=""))   
        }
        iKill <- iKill - 1
      }
      tParam <- param + 0.5 * delta
    }
    
    if(debug){ print(paste("Param[",i,"] =",tParam[i]))}
    result[i+1,] = as.double(tParam) 
    
  }
  
  if (debug){ print("/MakeSimplex") }
  return(result)
}

# [20:15, 19/06/21] Simon Williamson
# Thanks Craig, sorry it's taken me a while to get back round to this. 
# I think I understand how things are working from what you've explained. So I suppose the question is why have tolerance at all? Why not have each vertex extend or shrink until it is below the original vertex, rather than below the original vertex + tol?
# Trying to answer this myself, I suppose there's the possibility that some parameters cannot be improved upon, in which case the test parameter can at least shrink until it is minimally worse than the original? And that minimum is defined by tol?



# Make simplex 3 (for within-beat parameters only)
simplex.MakeSimplex3 <- function(ppg, param,f,inScale, directions=NULL, inTol=-1, optional=NULL, debug=FALSE, beat_vector = beat_vector, renal_param, dias_param = dias_param, sys_time, w){
  
  if(debug){print("MakeSimplex -- debug")}
  nPar <- length(param)
  nScale <- length(inScale)
  if (nScale == 0)
  {
    scale <- 1:nPar * 0 + 1
  } else if (nScale == 1){
    scale <- 1:nPar * 0 + inScale
  } else if (length(inScale) == nPar){
    scale <- inScale
  } else {
    #print("Invalid scale vector length")
    return("Error: Invalid scale vector length")
  }
  if (length(inTol) == 1 & inTol > 0){
    tol <- inTol[1]
  } else {
    tol <- min(1,f(data = ppg, params = param, beats = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w))    
  }    
  
  
  chiSq <- 1:(nPar+1) * 0.0
  chiSq[1] <- f(data = ppg, param, beats = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)
  if (debug){ print(paste("Root chi-squared:",chiSq[1]))}
  
  result <- matrix(nrow=nPar+1,ncol=nPar)
  result[1,] <- as.double(param)
  
  useDirections = !is.null(directions)
  if (useDirections){ useDirections <- nrow(directions) == nPar & ncol(directions) == nPar }
  
  for(i in c(1:4, 7, 10)){    # within-beat parameters only  
    if (debug){ print(paste("Parameter",i)) }
    tParam <- param
    
    # Pick a direction
    delta <- 1:nPar * 0
    if (useDirections){
      delta <- scale[i] * directions[i,]
    } else {
      delta[i] <- scale[i]
    }
    
    if(i == 3){
      delta <- delta/4
    }
    
    tParam <- param - delta                                   # This part tries tweaking each parameter up or down, 
    chiSqMinus <- f(data = ppg, params = tParam, beats = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)            # tParam = test parameter. 
    tParam <- param + delta                                   # The chisquare (goodness of fit) is calculated for each direction, 
    chiSq[i+1] <- f(data = ppg, params = tParam, beats = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)            # the direction with the smaller value is chosen. 
    
    if (debug){
      print("Select direction:")
      print(paste("chi^2(",param[i] - delta[i],") =",chiSqMinus))
      print(paste("chi^2(",param[i],") =",chiSq[1]))
      print(paste("chi^2(",param[i] + delta[i],") =",chiSq[i+1]))
      print("---")
    }
    
    if (chiSqMinus < chiSq[i+1]){
      delta <- -delta
      tParam <- param + delta
      chiSq[i+1] <- chiSqMinus
    }
    
    iKill <- 10    
    
    if (chiSq[i+1] < chiSq[1]){
      if (debug){ print("Extending as best point") }
      while (chiSq[i+1] < chiSq[1] + tol){                 # Chisquare keeps getting iterated here (for 10 iterations)
        delta <- 2*delta
        tParam <- param + delta
        oldScore <- chiSq[i+1]
        chiSq[i+1] <- f(data = ppg,tParam, beats = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)
        if (debug){ print(paste("chi^2(",tParam[i],") =",chiSq[i+1])) }
        if (chiSq[i+1] > oldScore){
          tParam <- param + 0.5*delta
          chiSq[i+1] <- oldScore
          break
        }
        #print(paste(i,"-",delta,":",chiSq[i+1]))
        iKill <- iKill - 1
        if (iKill < 0){
          break
        }
      }
    } else if (chiSq[i+1] < chiSq[1] + tol){
      if (debug){ print("Extending below tolerance") }
      while (chiSq[i+1] < chiSq[1] + tol){
        delta <- 2*delta
        tParam <- param + delta
        oldScore <- chiSq[i+1]
        chiSq[i+1] <- f(data = ppg, tParam, beats = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)
        if (debug){ print(paste("chi^2(",tParam[i],") =",chiSq[i+1])) }
        if (chiSq[i+1] - oldScore < oldScore - chiSq[1]){
          tParam <- param + 0.5*delta
          chiSq[i+1] <- oldScore
          break
        }
        iKill <- iKill - 1
        if (iKill < 0){
          #print("Failed to construct simplex")
          #return(paste("Error: param[",i,"]",sep=""))
          print(c("Failed to construct simplex within 10 iterations for parameter", i, "defaulting to inputted value"))
          tParam[i] <- param[i]
          break  # this was next 
        }
      }
    } else {
      if (debug){ print("Shrinking above tolerance") }
      while (chiSq[i+1] > chiSq[1] + tol){
        delta <- 0.5*delta
        tParam <- param + delta
        lastChiSq <- chiSq[i+1]
        chiSq[i+1] <- f(data = ppg,tParam, beats = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)
        if (debug){ print(paste("chi^2(",tParam[i],") =",chiSq[i+1])) }
        #print(paste(i,"-",delta,":",chiSq[i+1]))
        if (iKill < 0 & (chiSq[i+1]-chiSq[1]) > 0.75 * (lastChiSq-chiSq[1])){
          print(c("Failed to construct simplex within 10 iterations for parameter", i, "defaulting to inputted value"))
          #return(paste("Error: param[",i,"]",sep=""))
          tParam[i] <- param[i]
          break # this was next
        }
        iKill <- iKill - 1
      }
      tParam <- param + 0.5 * delta
    }
    
    if(debug){ print(paste("Param[",i,"] =",tParam[i]))}
    result[i+1,] = as.double(tParam)
  }
  
  if (debug){ print("/MakeSimplex") }
  return(result)
}



simplex.Run2 <- function(data = ppg,simplexParam = mat, f = model2.ChiSq3, optional=NULL, beat_vector = beat_vector, ms = simplex_iterations, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w, run = NULL){
  ########################################################################################################################################
  # Simplex.Run2 is the function that intiates the process of simplex relfection through the multi-dimensional landscape. 
  
  # Inputs: 
  # data ()
  # simplexParam (a matrix... )
  # f (the function used to assess goodness of fit by comparing the fitted wave to the original data wave)
  # optional 
  # beat_vector (an index of beats to be modeled and their x-coordinates to extract from the PPG time series)
  # renal_param (the intially identified renal timing, used as an anchor point for the model)
  # dias_param (the initially identified diastolic timing, used as an achor point for the model / about which the model is constrained)
  # sys_time (the timing of systole (about which the model is constrained))
  # w ()
  # run (indicates which simplex run is being conducted when printed)
  
  
  # Outputs:
  # Basically the set of parameters which best optimizes the function model2.ChiSq
  ########################################################################################################################################
  MAX_STEP <- ms                                               # Maximum number of allowed function evaluations (numerical recipes)
  FTOL <- 1e-5                                  # Tolerance, or fraction of tolerance possibly related to machine precision - need to clarify
  
  debugRtol <- 1:(MAX_STEP+1) * 0.0             # Not sure if we use any of these at any point... 
  debugMin <- 1:(MAX_STEP+1) * 0.0
  debugMax <- 1:(MAX_STEP+1) * 0.0
  
  result <- simplexParam                         # Passing in the 66*66 matrix, which will be the output once iterated on
  nPar <- ncol(result)                      
  chiSq <- 0:nPar * 0.0             # A separate chisq value exists for each parameter, so 66... 
  
  for (i in 1:(nPar+1)){                                    # Find out the ChiSq value for each row from result, there are 66 + 1 rows
    chiSq[i] <- f(data, params = NULL, optional=NULL, a = result[i, ], beats = beat_vector,
                  renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)
  }                         # Having this vector gives us the values of each point in the simplex
  
  for (iStep in 1:MAX_STEP){                             # beginning of downhill simplex
    extrema <- simplex.SortHighLow(chiSq)                # Finds the results which give the highest, 2nd highest and lowest ChiSq
    low <- extrema[1]
    nHigh <- extrema[2] 
    high <- extrema[3]
    
   if(!is.null(run)){        # just prints run number and iteration number, so you have an idea of it's progress when running it
      print(run)
    }
    print(iStep)
    
    chiSqMax <- chiSq[high]                     # redefine highest and lowest points...
    chiSqMin <- chiSq[low]                      # These should be the ChiSq values for individual rows (the best and worst points), so how can it be compared to the 'score' that resresents the entire new simplex?
    
    print(chiSqMax)
    
    #print(paste("chi^2_min =",chiSqMin))
    #print(paste("argMax = ",high,"[",chiSqMax,"]",sep=""))
    
    rtol <- 2 * (chiSqMax - chiSqMin)/(chiSqMax + chiSqMin + 1e-10)   # measure of how much better high is from low...
    if (rtol < FTOL){
      bestParam <- result[low,]                     # Presumably if the difference in ChiSq (max vs min) is significant, 
      result[low,] <- result[1,]                    # the result that was changed to give the lowest ChiSq gets designated 'best Param'
      result[1,] <- bestParam                       # The best performing row gets upgraded to first row (swapped with what is there currently)
      return(result) 
    }
    
                                                     # So at this point you have input the matrix, identified chiSq values for each row, and moved the row 
                                                     # that generates the best Chisq to the top (at least for iteration 1). 
    
    debugRtol[iStep] <- rtol                         # ? necessary lines?
    debugMin[iStep] <- chiSqMin
    debugMax[iStep] <- chiSqMax
    
    factor <- -1       # not sure what this does
    node <- simplex.HypoCentre(result,high)        # Hypocentre outputs all the parameters that are not the worst  - IS THIS A SINGLE ROW OR A MATRIX?
    apex <- result[high,]                          # Apex is the worst row / point on the simplex
    test <- node - (apex - node)                   # This represents the flipping of the triangle; the whole parameter set is reversed in the direction away from the worst ChiSq point (literally, we are subtracting the difference between each row and the worst row from each row, causing all rows to be less similar to the worst ro)
    score <- f(data, params = rep(0, 12),optional=optional, a = test, beats = beat_vector,
               renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)   # QUESTION: is this a single value? Does it evaluate every row of the simplex?
    
    if (score < chiSqMin){                          # If flipping improves the ChiSq, try extending further in the same direction (increase the scale by 2) i.e reflection and expansion
      test2 <- node - 2 * (apex - node)
      score2 <- f(data, params = rep(0, 12),optional=optional, a = test2, beats = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)
      if (score2 >= score){                       # If reflecting a further distance is better than reflecting alone, do that
        # Reflect
        #print(paste("Reflecting",high,": chi^2 ",chiSqMax,"->",score,sep=""))
        result[high,] <- test       # test is replacing one row, so is test just one row?
        chiSq[high] <- score
      } else {
        # Reflect and grow
        #print(paste("Reflect-stretching",high,": chi^2 ",chiSqMax,"->",score2,sep=""))
        result[high,] <- test2
        chiSq[high] <- score2
      }
    } else if (score >= chiSq[nHigh]) {              # If reflecting is not beneficial, try shrinking instead of reflecting
      # Test for shrink with optional reflection
      factor <- 0.5
      if (score < chiSqMax)
      {
        factor <- -0.5
      }
      test2 <- node + factor * (apex - node)
      score2 <- f(data, params = rep(0, 12),optional=optional, a = test2, beats = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)
      if (score2 < chiSq[nHigh]){
        # Shrink (possibly reflecting)
        #print(paste("Shrinking",high,": chi^2 ",chiSqMax,"->",score2,sep=""))
        result[high,] <- test2
        chiSq[high] <- score2
      } else {
        # Shrink all
        for (i in 1:(nPar+1)){
          if (i != low){
            result[i,] <- 0.5 * (result[i,] + result[low,])
            chiSq[i] <- f(data, params = rep(0, 12),optional=optional, a = result[i, ], beats = beat_vector, renal_param = renal_param, dias_param = dias_param, sys_time = sys_time, w = w)
          }
        }
        #print(paste("General contraction: chi^2 ",chiSqMax,"->",max(chiSq),sep=""))
      }
    } else {
      # Reflect
      #print(paste("Reflecting*",high,": chi^2 ",chiSqMax,"->",score,sep=""))
      result[high,] <- test
      chiSq[high] <- score
    }
  }
  
  extrema <- simplex.SortHighLow(chiSq)
  low <- extrema[1]
  bestParam <- result[low,]
  result[low,] <- result[1,]
  result[1,] <- bestParam
  
  chiSqMax <- chiSq[extrema[3]]
  chiSqMin <- chiSq[low]
  rtol <- 2 * (chiSqMax - chiSqMin)/(chiSqMax + chiSqMin + 1e-10)   # Why does rtol need to be redefined here?
  debugRtol[MAX_STEP+1] <- rtol
  debugMin[MAX_STEP+1] <- chiSqMin      # Are these debug lines needed?
  debugMax[MAX_STEP+1] <- chiSqMax
  # plot(debugMax,type='l')
  # lines(debugMin)
  
  
  print(paste("Terminated downhill simplex after",MAX_STEP,"iterations."))
  print(paste("rtol =",rtol))
  return(result)
}


simplex.HypoCentre <- function(mat_Param,index){
  nPar <- ncol(mat_Param)
  
  result <- 1:nPar * 0.0
  for (i in 1:(nPar+1)){
    if (i != index){
      result <- result + mat_Param[i,]
    }
  }
  return( result / nPar )
}


simplex.SortHighLow <- function(vec_ChiSq){
  nPar <- length(vec_ChiSq)
  
  low <- 1
  high <- 1
  nHigh <- 2
  if (vec_ChiSq[2] > vec_ChiSq[1]){
    high <- 2
    nHigh <- 1
  }
  
  for (i in 2:nPar){
    if (vec_ChiSq[i] < vec_ChiSq[low]){
      low <- i
    }
    if (vec_ChiSq[i] > vec_ChiSq[high]){
      nHigh <- high
      high <- i
    }
    if (i != high & vec_ChiSq[i] > vec_ChiSq[nHigh]){
      nHigh <- i
    }
  }
  
  return(c(low,nHigh,high))  
}


# This is an exceptionally inefficient use of space, and needs to be revisited:

PlotRejects <- function(rejected_waves_list1, rejected_waves_list3){
  
  if(length(rejected_waves_list3) > 0){
    len <- 2
  }else{
    len <- 1
  }
  
  participant_extra_long_waves <- c()
  for(k in 1:len){
    
    if(k == 1){
      new_vec <- rejected_waves_list1
    }else{
      new_vec <- rejected_waves_list3
    }
    
    extra_long_waves <- c()
    for(i in 1:length(new_vec)){
      if(!is.null(new_vec[[i]][[1]])){
        extra_long_waves[[i]] <- new_vec[[i]][[1]]
      }
    }
    non_null_names <- which(!sapply(extra_long_waves, is.null))
    extra_long_waves <- extra_long_waves[non_null_names]
    if(length(non_null_names) > 0){names(extra_long_waves) <- non_null_names}
    
    if(length(extra_long_waves) > 0){
      for(j in 1:length(extra_long_waves)){
        
        no_of_rejected_waves <- length(extra_long_waves[[j]])
        
        paticip <- as.numeric(rownames(summary(extra_long_waves[j])))
        
        participant_extra_long_waves[paticip] <- length(participant_extra_long_waves[paticip]) + no_of_rejected_waves 
      }
    }
  }
  if(length(participant_extra_long_waves) > 0){
    for(i in 1:length(participant_extra_long_waves)){
      if(is.na(participant_extra_long_waves[i])){
        participant_extra_long_waves[i] <- 0
      }
    }
  }

  
  participant_extra_short_waves <- c()
  for(k in 1:len){
    
    if(k == 1){
      new_vec <- rejected_waves_list1
    }else{
      new_vec <- rejected_waves_list3
    }
    
    extra_short_waves <- list()
    for(i in 1:length(new_vec)){
      if(!is.null(new_vec[[i]][[2]])){
        extra_short_waves[[i]] <- new_vec[[i]][[2]]
      }
    }
    non_null_names <- which(!sapply(extra_short_waves, is.null))
    extra_short_waves <- extra_short_waves[non_null_names]
    if(length(non_null_names) > 0){names(extra_short_waves) <- non_null_names}
    
    if(length(extra_short_waves) > 0){
      for(j in 1:length(extra_short_waves)){
        
        no_of_rejected_waves <- length(extra_short_waves[[j]])
        
        paticip <- as.numeric(rownames(summary(extra_short_waves[j])))
        
        participant_extra_short_waves[paticip] <- length(participant_extra_short_waves[paticip]) + no_of_rejected_waves 
      }
    }
  }
  if(length(participant_extra_short_waves) > 0){
    for(i in 1:length(participant_extra_short_waves)){
      if(is.na(participant_extra_short_waves[i])){
        participant_extra_short_waves[i] <- 0
      }
    }
  }
  
  participant_double_segments <- c()
  for(k in 1:len){
    
    if(k == 1){
      new_vec <- rejected_waves_list1
    }else{
      new_vec <- rejected_waves_list3
    }
    
    double_segments <- list()
    for(i in 1:length(new_vec)){
      if(!is.null(new_vec[[i]][[3]])){
        double_segments[[i]] <- new_vec[[i]][[3]]
      }
    }
    non_null_names <- which(!sapply(double_segments, is.null))
    double_segments <- double_segments[non_null_names]
    if(length(non_null_names) > 0){names(double_segments) <- non_null_names}
    
    if(length(double_segments) > 0){
      for(j in 1:length(double_segments)){
        
        no_of_rejected_waves <- length(double_segments[[j]])
        
        paticip <- as.numeric(rownames(summary(double_segments[j])))
        
        participant_double_segments[paticip] <- length(participant_double_segments[paticip]) + no_of_rejected_waves 
      }
    }
  }
  if(length(participant_double_segments) > 0){
    for(i in 1:length(participant_double_segments)){
      if(is.na(participant_double_segments[i])){
        participant_double_segments[i] <- 0
      }
    }
  }
 
  
  
  participant_systolic_endings <- c()
  for(k in 1:len){
    
    if(k == 1){
      new_vec <- rejected_waves_list1
    }else{
      new_vec <- rejected_waves_list3
    }
    
    systolic_endings <- list()
    for(i in 1:length(new_vec)){
      if(!is.null(new_vec[[i]][[4]])){
        systolic_endings[[i]] <- new_vec[[i]][[4]]
      }
    }
    non_null_names <- which(!sapply(systolic_endings, is.null))
    systolic_endings <- systolic_endings[non_null_names]
    if(length(non_null_names) > 0){names(systolic_endings) <- non_null_names}
    
    if(length(systolic_endings) > 0){
      for(j in 1:length(systolic_endings)){
        
        no_of_rejected_waves <- length(systolic_endings[[j]])
        
        paticip <- as.numeric(rownames(summary(systolic_endings[j])))
        
        participant_systolic_endings[paticip] <- length(participant_systolic_endings[paticip]) + no_of_rejected_waves 
      }
    }
    
  }
  if(length(participant_systolic_endings) > 0){
    for(i in 1:length(participant_systolic_endings)){
      if(is.na(participant_systolic_endings[i])){
        participant_systolic_endings[i] <- 0
      }
    }
  }
 
  
  
  participant_drops_below_o <- c()
  for(k in 1:len){
    
    if(k == 1){
      new_vec <- rejected_waves_list1
    }else{
      new_vec <- rejected_waves_list3
    }
    
    drops_below_o <- list()
    for(i in 1:length(new_vec)){
      if(!is.null(new_vec[[i]][[5]])){
        drops_below_o[[i]] <- new_vec[[i]][[5]]
      }
    }
    non_null_names <- which(!sapply(drops_below_o, is.null))
    drops_below_o <- drops_below_o[non_null_names]
    if(length(non_null_names) > 0){names(drops_below_o) <- non_null_names}
    
    if(length(drops_below_o) > 0){
      for(j in 1:length(drops_below_o)){
        
        no_of_rejected_waves <- length(drops_below_o[[j]])
        
        paticip <- as.numeric(rownames(summary(drops_below_o[j])))
        
        participant_drops_below_o[paticip] <- length(participant_drops_below_o[paticip]) + no_of_rejected_waves 
      }
    }
   
  }
  if(length(participant_drops_below_o) > 0){
    for(i in 1:length(participant_drops_below_o)){
      if(is.na(participant_drops_below_o[i])){
        participant_drops_below_o[i] <- 0
      }
    }
  }

  
  participant_hrsd_waves <- c()
  for(k in 1:len){
    
    if(k == 1){
      new_vec <- rejected_waves_list1
    }else{
      new_vec <- rejected_waves_list3
    }
    
    hrsd_waves <- list()
    for(i in 1:length(new_vec)){
      if(!is.null(new_vec[[i]][[6]])){
        hrsd_waves[[i]] <- new_vec[[i]][[6]]
      }
    }
    non_null_names <- which(!sapply(hrsd_waves, is.null))
    hrsd_waves <- hrsd_waves[non_null_names]
    if(length(non_null_names) > 0){names(hrsd_waves) <- non_null_names}
    
    if(length(hrsd_waves) > 0){
      for(j in 1:length(hrsd_waves)){
        
        no_of_rejected_waves <- length(hrsd_waves[[j]])
        
        paticip <- as.numeric(rownames(summary(hrsd_waves[j])))
        
        participant_hrsd_waves[paticip] <- length(participant_hrsd_waves[paticip]) + no_of_rejected_waves 
      }
    }

  }
  if(length(participant_hrsd_waves) > 0){
    for(i in 1:length(participant_hrsd_waves)){
      if(is.na(participant_hrsd_waves[i])){
        participant_hrsd_waves[i] <- 0
      }
    }
  }

  
  participant_outlier_waves <- c()
  for(k in 1:len){
    
    if(k == 1){
      new_vec <- rejected_waves_list1
    }else{
      new_vec <- rejected_waves_list3
    }
    
    outlier_waves <- list()
    for(i in 1:length(new_vec)){
      if(!is.null(new_vec[[i]][[7]])){
        outlier_waves[[i]] <- new_vec[[i]][[7]]
      }
    }
    non_null_names <- which(!sapply(outlier_waves, is.null))
    outlier_waves <- outlier_waves[non_null_names]
    if(length(non_null_names) > 0){names(outlier_waves) <- non_null_names}
    
    if(length(outlier_waves) > 0){
      for(j in 1:length(outlier_waves)){
        
        no_of_rejected_waves <- length(outlier_waves[[j]])
        
        paticip <- as.numeric(rownames(summary(outlier_waves[j])))
        
        participant_outlier_waves[paticip] <- length(participant_outlier_waves[paticip]) + no_of_rejected_waves 
      }
    }
    
  }
  if(length(participant_outlier_waves) > 0){
    for(i in 1:length(participant_outlier_waves)){
      if(is.na(participant_outlier_waves[i])){
        participant_outlier_waves[i] <- 0
      }
    }
  }

  # Make them all the same length:
  if(length(participant_extra_long_waves) < length(Participants)){
    diff <- length(Participants) - length(participant_extra_long_waves)
    participant_extra_long_waves <- c(participant_extra_long_waves, rep(0, diff))
  }
  if(length(participant_extra_short_waves) < length(Participants)){
    diff <- length(Participants) - length(participant_extra_short_waves)
    participant_extra_short_waves <- c(participant_extra_short_waves, rep(0, diff))
  }
  if(length(participant_double_segments) < length(Participants)){
    diff <- length(Participants) - length(participant_double_segments)
    participant_double_segments <- c(participant_double_segments, rep(0, diff))
  }
  if(length(participant_systolic_endings) < length(Participants)){
    diff <- length(Participants) - length(participant_systolic_endings)
    participant_systolic_endings <- c(participant_systolic_endings, rep(0, diff))
  }
  if(length(participant_drops_below_o) < length(Participants)){
    diff <- length(Participants) - length(participant_drops_below_o)
    participant_drops_below_o <- c(participant_drops_below_o, rep(0, diff))
  }
  if(length(participant_hrsd_waves) < length(Participants)){
    diff <- length(Participants) - length(participant_hrsd_waves)
    participant_hrsd_waves <- c(participant_hrsd_waves, rep(0, diff))
  }
  if(length(participant_outlier_waves) < length(Participants)){
    diff <- length(Participants) - length(participant_outlier_waves)
    participant_outlier_waves <- c(participant_outlier_waves, rep(0, diff))
  }
  # Find total of all rejected beats:
  total_rejected_beats <- participant_extra_long_waves + participant_extra_short_waves + participant_double_segments + 
    participant_systolic_endings + participant_drops_below_o + participant_hrsd_waves + participant_outlier_waves
  
  
  # Plot:
  plot(participant_extra_long_waves, t = "l", ylim = c(0, 30), xlim = c(1, 112), ylab = "rejected beats (absolute)", xlab = "participants", lty = "dotted", lwd = 1.5)
  lines(participant_extra_short_waves, col = "red", lty = "dotted", lwd = 1.5)
  lines(participant_double_segments, col = "blue", lty = "dotted", lwd = 1.5)
  lines(participant_systolic_endings, col = "green", lty = "dotted", lwd = 1.5)
  lines(participant_drops_below_o, col = "orange", lty = "dotted", lwd = 1.5)
  lines(participant_hrsd_waves, col = "brown", lty = "dotted", lwd = 1.5)
  lines(participant_outlier_waves, col = "purple", lty = "dotted", lwd = 1.5)
  lines(total_rejected_beats, lwd = 1)
  
}


PlotWavesCarriedForward <- function(waves_carried_forward1, waves_carried_forward3){
  
  test_vec1 <- waves_carried_forward1
  waves_carried_over <- c()
  for(i in 1:length(test_vec1)){
    if(!is.null(test_vec1[[i]])){
      waves_carried_over[i] <- test_vec1[[i]]
    }
  }
  
  if(length(waves_carried_forward3) > 0){
    test_vec2 <- waves_carried_forward3
    waves_carried_over2 <- c()
    for(i in 1:length(test_vec2)){
      if(!is.null(test_vec2[[i]])){
        waves_carried_over2[i] <- test_vec2[[i]]
      }
    }
    waves_carried_over <- c(waves_carried_over, waves_carried_over2)
  }
  
  
  # Currently representing all 2mg times series:
  hist(waves_carried_over, breaks = 30, xlim = c(0, 200))
  
}
